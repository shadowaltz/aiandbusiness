---
icon: file-image
---

# Image Generation/Synthesis

In the recent surge of AI applications, the image generation field has risen in popularity alongside large language models. Stable Diffusion has played a crucial role in this development.



### Learning Resources:

<table data-view="cards"><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>OverView of generative AI animation techniques</td><td><a href="https://diffusionpilot.blogspot.com/2023/09/overview-ai-animation.html">https://diffusionpilot.blogspot.com/2023/09/overview-ai-animation.html</a></td><td></td></tr><tr><td>How Stable Diffusion Works from Chris McCormick</td><td><a href="https://mccormickml.com/2022/12/21/how-stable-diffusion-works/">https://mccormickml.com/2022/12/21/how-stable-diffusion-works/</a></td><td></td></tr><tr><td>How Diffusion Models Work from DeepLearning</td><td><a href="https://www.deeplearning.ai/short-courses/how-diffusion-models-work/">https://www.deeplearning.ai/short-courses/how-diffusion-models-work/</a></td><td></td></tr><tr><td>Niji Academy</td><td><a href="https://www.niji.academy/work/lecture">https://www.niji.academy/work/lecture</a></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table>



{% hint style="warning" %}
### AI Data Poisoning Tool - Nightshade

Nightshade is an innovative tool developed by researchers at the University of Chicago to protect artists' work from being used without permission in AI training datasets. It works by adding subtle modifications to images that are imperceptible to humans but can disrupt AI models that attempt to learn from or replicate these images. When AI systems train on Nightshade-protected images, they may produce distorted or incorrect outputs, effectively "poisoning" the model and deterring unauthorized use of artists' creations.

[https://nightshade.cs.uchicago.edu/whatis.html](https://nightshade.cs.uchicago.edu/whatis.html)
{% endhint %}



