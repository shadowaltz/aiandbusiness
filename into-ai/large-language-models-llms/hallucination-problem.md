# Hallucination Problem

## <mark style="color:purple;">What is AI Hallucination?</mark>

{% hint style="info" %}
<mark style="color:red;">**AI hallucination**</mark> occurs when a <mark style="color:red;">**Generative AI Model generates false, misleading, or illogical information and presents it as fact**</mark>. This issue arises because LLMs are trained on vast amounts of text data and use statistical patterns to predict the next word in a sequence, rather than understanding the underlying reality of the content they generate.
{% endhint %}



### Causes of AI Hallucinations

{% hint style="success" %}
Large language models hallucinate—producing confident but false outputs—because their training and evaluation processes reward guessing over admitting uncertainty.
{% endhint %}

{% embed url="https://openai.com/index/why-language-models-hallucinate/" %}



### Hallucination Leaderboard

[https://github.com/vectara/hallucination-leaderboard](https://github.com/vectara/hallucination-leaderboard)

{% embed url="https://github.com/vectara/hallucination-leaderboard/raw/main/img/hallucination_rates_with_logo.png" %}

### **Some Real Life Cases:**

#### Case 1: Lawyer Citing Fake Cases Generated by ChatGPT

In 2023, a New York lawyer, Steven Schwartz, used ChatGPT to draft a legal brief for a personal injury case. The brief included several fictitious court cases fabricated by the AI, which opposing counsel could not verify. When challenged, Schwartz and his colleague, Peter LoDuca, doubled down on their assertions until the court ordered them to provide the cases. Upon failing to do so, they were fined $5,000. Furthermore, the court mandated that any future filings involving generative AI content must explicitly disclose such use to ensure accuracy checks​ ([ABA Journal](https://www.abajournal.com/web/article/lawyers-who-doubled-down-and-defended-chatgpts-fake-cases-must-pay-5k-judge-says))​​ ([Law.com](https://www.law.com/newyorklawjournal/2023/06/22/judge-imposes-5k-fine-on-lawyers-who-submitted-chatgpt-generated-fake-case-citations/))​.

#### Case 2: AI-Generated Misinformation

AI-generated false information can spread rapidly, leading to public misunderstanding. An example of this occurred with Google’s Bard chatbot, which incorrectly claimed that the James Webb Space Telescope had taken the first image of an exoplanet. This misinformation quickly disseminated after its release, demonstrating the potential for AI to propagate erroneous news​.

{% embed url="https://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-demo" %}

These cases underscore the importance of rigorous verification processes and the responsible use of AI technology to prevent the spread of false information and ensure the integrity of legal documents.

### Mitigation Techniques

1. **Use Better Large Language Models:** Whenever possible, utilize more powerful large models, as they tend to produce fewer hallucinations.
2. **Use AI Search Engines:** Employ applications like Perplexity, which are optimized for search tasks using large models. These applications provide answers based on internet data content. (\*Although they can still have hallucinations, the chances are significantly lower.)
3. **Human-in-the-Loop:** Have humans meticulously verify AI-generated indexes, cases, and other content.
4. **Refine Your Prompts:** Use more detailed and sophisticated prompts and prompt structures to obtain more accurate results.
5. **Use Advanced Reasoning LLM:** Choose models specifically trained or fine-tuned for logical reasoning and step-by-step problem-solving tasks, such as GPT-4 with chain-of-thought (CoT) prompting or Claude 3, which systematically reduce hallucinations through explicit reasoning processes.



**Here is a page on the OpenAI Cookbook that provides guidelines and examples for developing guardrails to prevent hallucinations in AI models.**

{% embed url="https://cookbook.openai.com/examples/developing_hallucination_guardrails" %}

{% hint style="success" %}
**In summary, always meticulously verify any content generated by large models before using it.**
{% endhint %}

***

### <mark style="color:orange;">Hallucination Detector</mark>

[https://demo.exa.ai/hallucination-detector](https://demo.exa.ai/hallucination-detector)

{% embed url="https://github.com/exa-labs/exa-hallucination-detector/raw/main/public/opengraph-image.jpg" %}



