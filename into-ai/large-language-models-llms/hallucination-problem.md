# Hallucination Problem

## <mark style="color:purple;">What is AI Hallucination?</mark>

{% hint style="info" %}
<mark style="color:red;">**AI hallucination**</mark> occurs when a <mark style="color:red;">**Generative AI Model generates false, misleading, or illogical information and presents it as fact**</mark>. This issue arises because LLMs are trained on vast amounts of text data and use statistical patterns to predict the next word in a sequence, rather than understanding the underlying reality of the content they generate.
{% endhint %}



### Causes of AI Hallucinations

There are several reasons for AI hallucinations. Some large models suffer from insufficient training data, while others have low-quality training data containing errors. Additionally, some models were trained earlier and lacked updated information. User prompts that are unclear can also lead to hallucinations. Furthermore, chatbots designed to maintain conversation flow might generate inaccurate information in the absence of correct data (it is easy for an LLm) to fulfill their primary task of keeping the dialogue going.



### **Some Real Life Cases:**

#### Case 1: Lawyer Citing Fake Cases Generated by ChatGPT

In 2023, a New York lawyer, Steven Schwartz, used ChatGPT to draft a legal brief for a personal injury case. The brief included several fictitious court cases fabricated by the AI, which opposing counsel could not verify. When challenged, Schwartz and his colleague, Peter LoDuca, doubled down on their assertions until the court ordered them to provide the cases. Upon failing to do so, they were fined $5,000. Furthermore, the court mandated that any future filings involving generative AI content must explicitly disclose such use to ensure accuracy checks​ ([ABA Journal](https://www.abajournal.com/web/article/lawyers-who-doubled-down-and-defended-chatgpts-fake-cases-must-pay-5k-judge-says))​​ ([Law.com](https://www.law.com/newyorklawjournal/2023/06/22/judge-imposes-5k-fine-on-lawyers-who-submitted-chatgpt-generated-fake-case-citations/))​.

#### Case 2: AI-Generated Misinformation

AI-generated false information can spread rapidly, leading to public misunderstanding. An example of this occurred with Google’s Bard chatbot, which incorrectly claimed that the James Webb Space Telescope had taken the first image of an exoplanet. This misinformation quickly disseminated after its release, demonstrating the potential for AI to propagate erroneous news​.

{% embed url="https://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-demo" %}

These cases underscore the importance of rigorous verification processes and the responsible use of AI technology to prevent the spread of false information and ensure the integrity of legal documents.

### Mitigation Techniques

1. **Use Better Large Language Models:** Whenever possible, utilize more powerful large models, as they tend to produce fewer hallucinations.
2. **Use AI Search Engines:** Employ applications like Perplexity, which are optimized for search tasks using large models. These applications provide answers based on internet data content. (\*Although they can still have hallucinations, the chances are significantly lower.)
3. **Human-in-the-Loop:** Have humans meticulously verify AI-generated indexes, cases, and other content.
4. **Refine Your Prompts:** Use more detailed and sophisticated prompts and prompt structures to obtain more accurate results.



**Here is a page on the OpenAI Cookbook that provides guidelines and examples for developing guardrails to prevent hallucinations in AI models.**

{% embed url="https://cookbook.openai.com/examples/developing_hallucination_guardrails" %}

{% hint style="success" %}
**In summary, always meticulously verify any content generated by large models before using it.**
{% endhint %}





